[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"RBookdown tutorial condenses analysis Imbens & Xu (2024). reflect five key lessons methodological literature following LaLonde’s 1986 study:central role unconfoundednessThe importance overlap covariate distributionsThe focus propensity scores constructing modern estimatorsCredibility validation exercisesInvestigation alternative estimandsRevisiting LaLonde’s analysis, present modern methods enhance causal inference using data compiled Rajeev Dehejia Sadek Wahba (Dehejia Wahba 1999, 2002) survey data lottery players Imbens, Rubin, Sacerdote (2001). Hereafter, denote former dataset LaLonde-Dehejia-Wahba (LDW) data, latter IRS data. findings highlight causal inference observational data demands rigorous ( ) examination assignment process, ( ii ) assessment overlap, ( iii ) execution validation checks. tutorial streamlines methodology used Imbens & Xu (2024), making easier readers replicate study apply techniques research. addition, supplement complementary application analytics original LaLonde (1986) data LDW analysis demonstrate validate methodologies.Reference: Imbens, Guido Yiqing Xu (2024). “LaLonde (1986) Nearly Four Decades: Lessons Learned.” Working Paper, Stanford University.Acknowledgement: thank Zihan Xie Jinwen Wu excellent research assistance, makes tutorial possible.","code":""},{"path":"getting-started.html","id":"getting-started","chapter":"1 Getting Started","heading":"1 Getting Started","text":"","code":""},{"path":"getting-started.html","id":"installation","chapter":"1 Getting Started","heading":"1.1 Installation","text":"Several R packages required subsequent data analysis \nvisualization. code checks required messages \ninstalls missing ones. installation, call load \npackages.Packages: “haven”, “labelled”, “estimatr”, “grf”, “ggplot2”,\n“Matching”, “hbal”, “CBPS”, “DoubleML”, “mlr3learners”, “fixest”, “qte”,\n“sensemakr”","code":"\n# required packages\npackages <- c(\"haven\", \"labelled\", \"Matching\", \"grf\", \"sensemakr\", \"qte\",\n    \"estimatr\", \"CBPS\", \"hbal\", \"DoubleML\", \"mlr3learners\", \"mlr3\",\"fixest\", \"ggplot2\")\n\n# install packages\ninstall_all <- function(packages) {\n  installed_pkgs <- installed.packages()[, \"Package\"]\n  for (pkg in packages) {\n    if (!pkg %in% installed_pkgs) {\n      install.packages(pkg)\n    }\n  }\n}\n\ninstall_all(packages)\n\n# load packages\nlibrary(haven)\nlibrary(labelled)\nlibrary(grf)\nlibrary(Matching)\nlibrary(estimatr)\nlibrary(hbal)\nlibrary(CBPS)\nlibrary(DoubleML)\nlibrary(mlr3learners)\nlibrary(mlr3)\nlibrary(fixest)\nlibrary(ggplot2)\nlibrary(qte)\nlibrary(sensemakr)"},{"path":"getting-started.html","id":"wrapper-functions","chapter":"1 Getting Started","heading":"1.2 Wrapper Functions","text":"Next, outline 10 wrapper functions designed address 6 research\nobjectives. table offers brief overview category.\nClicking “” direct subsequent section detailed\nexplanations well full code.","code":""},{"path":"getting-started.html","id":"overview","chapter":"1 Getting Started","heading":"1.2.1 Overview","text":"","code":""},{"path":"getting-started.html","id":"plot_hist-assess_overlap","chapter":"1 Getting Started","heading":"1.2.2 plot_hist & assess_overlap","text":"plot_hist visualizes distribution propensity scores across\ntreated control groups, options adjust odds density.assess_overlap estimates overlap propensity scores \ntreatment group control group. fits probability forest \nestimate propensity scores based covariates (cov) treatment\nindicators input dataset. , function adjusts propensity\nscores close zero facilitate calculations. Finally, \ncalls plot_hist visualize distribution propensity scores\nlog odds, depending odds parameter.ArgumentsDatadata: targeted dataset.var: variable interest plot.treat: (binary) treatment indicator documented dataset\n(usually 0 control 1 treated).Analysisodds: TRUE, function transforms variable log odds.cov: Covariates used estimate propensity score.num.trees: Number trees use probability forest. \nNULL, default used.seed: Seed reproducibility.Plottingbreaks, density, xlim, ylim, xlab: Parameters \nhistogram aesthetics scaling.text.size: size text additional information \nplot.","code":"\nplot_hist <- function(data, var, treat, main = NULL, odds = FALSE,\n                      breaks = 40, density = TRUE, xlim = NULL, ylim = NULL,\n                      xlab = NULL, text.size = 0.8) {\n  ntr <- sum(data[, treat] == 1)\n  nco <- sum(data[, treat] == 0)\n  if (odds == TRUE) {\n    data[, var] <- log(data[, var]/(1-data[, var]))\n    if (is.null(xlab) == TRUE) {xlab <- \"Log Odds\"}\n  } else {\n    if (is.null(xlab) == TRUE) {xlab <- \"Propensity Score\"}\n  }\n  if (is.null(xlim)) {\n    if (odds == TRUE) {\n      xlim <- range(data[, var])\n      cat(xlim)\n    } else {\n      xlim <- c(0,1)\n    }\n  }\n  intervals <- seq(xlim[1], xlim[2], length.out = breaks + 1)\n  h0 <- as.numeric(table(cut(data[data[,treat]==0, var],\n                             breaks = intervals, include.lowest = TRUE)))\n  h1 <- as.numeric(table(cut(data[data[,treat]==1, var],\n                             breaks = intervals, include.lowest = TRUE)))\n  if (density == TRUE) {\n    h0 <- h0/sum(h0); h1 <- h1/sum(h1)\n  }\n  s <- cbind.data.frame(h0, h1)\n  if (is.null(ylim)) {\n    ylim.max <- max(s$h0, s$h1) * 1.2\n    ylim <- c(-ylim.max, ylim.max)\n  }\n  par(mar = c(4, 4, 1, 1))\n  barplot(s$h0 * -1, names.arg = sprintf(\"%.2f\", intervals[-1]),\n          col = \"#AAAAAA80\", main = main, cex.lab = 1.3,\n          ylim = ylim, xlab = xlab, cex.axis = 1.2, cex.names = 1.2,\n          ylab = \"Density\", border = NA, axes = TRUE)\n  barplot(s$h1, col = \"#ff000080\", add = TRUE,\n          border = NA, cex.axis = 1.2)\n  abline(h = 0, col = \"gray60\", lty = 2, lwd = 1.5)\n  axis(1, at = seq(1, 60, length.out = breaks/2), labels = FALSE)\n  usr <- par(\"usr\")\n  user_x <- usr[1] + 0.03 * (usr[2] - usr[1])\n  user_y <- usr[3] + 0.92 * (usr[4] - usr[3])\n  text(user_x, user_y, paste(\"Ntr = \", ntr), pos = 4, cex = text.size)\n  text(user_x, user_y - 0.05 * (usr[4] - usr[3]), paste(\"Nco = \", nco),\n       pos = 4, cex = text.size)\n  box()\n}\nassess_overlap <- function(data, treat, cov, odds = TRUE, num.trees = NULL, seed = 1234, breaks = 50, xlim = NULL, ylim = NULL) {\n  if(is.null(num.trees))\n  {\n    p.forest1 <- probability_forest(X = data[, cov],\n                                    Y = as.factor(data[,treat]), seed = seed)\n  }\n  else\n  {\n    p.forest1 <- probability_forest(X = data[, cov],\n                                    Y = as.factor(data[,treat]), seed = seed, num.trees = num.trees)\n  }\n  data$ps_assoverlap <- p.forest1$predictions[,2]\n  #range(lcps.plus$ps)\n  data$ps_assoverlap[which(abs(data$ps_assoverlap) <= 1e-7)] <- 1e-7\n  #range(lcps.plus$ps)\n  if(odds == TRUE)\n  {\n    plot_hist(data, \"ps_assoverlap\", treat, odds = TRUE, breaks = breaks,\n          density = TRUE, main = \"\", xlim = xlim, ylim = ylim)\n  }\n  else\n  {\n    plot_hist(data, \"ps_assoverlap\", treat, odds = FALSE, breaks = breaks,\n              density = TRUE, main = \"\", xlim = c(0, 1), ylim = ylim)\n  }\n  return(data)\n}"},{"path":"getting-started.html","id":"psmatch","chapter":"1 Getting Started","heading":"1.2.3 psmatch","text":"psmatch function matches observations treatment group \ncontrol group according propensity scores. matching\nprocedure executed one--one basis without replacement. \nfunction yields subset original dataset \nmatched cases.ArgumentsThe parameters used plot_hist assess_overlap carried\ninputs psmatch function. Note , \ndifferentiate psmatch using Y represents outcome variable\ninterest. Additional parameters :replace: boolean indicating whether sampling controls \nreplacement (default FALSE).estimand: estimand estimated, defaulting ATT.","code":"\npsmatch <- function(data, Y, treat, cov, num.trees = 4000, seed = 1234, replace = FALSE, estimand = \"ATT\")\n{\n  set.seed(seed) # need to set seed b/c tie-breaking is random\n  data$psmatch <- probability_forest(X = data[, cov],\n                                Y = as.factor(data[, treat]), seed = seed, num.trees = num.trees)$predictions[,2]\n  mout <- Match(Y = data[,Y], Tr = data[,treat], X = data$psmatch, estimand = estimand, M = 1,\n                BiasAdjust = FALSE, replace=replace, ties = FALSE)\n  data <- data[c(mout$index.treated, mout$index.control), ]\n  return(data)\n}"},{"path":"getting-started.html","id":"estimate_all-and-plot_coef","chapter":"1 Getting Started","heading":"1.2.4 estimate_all and plot_coef","text":"estimate_all comprehensive tool estimating Average\nTreatment Effect Treated (ATT) observational data. , \ncondensed several estimates :Difference MeansRegressionOaxaca Blinder (OM:Reg) Generalized Random Forests (OM:GRF) \noutcome model1: 5 nearest neighbor matching bias correction, propensity\nscore matchingInverse Probability Weighting (IPW), Covariate Balancing Propensity\nScore(CBPS), Entropy BalancingDouble/debiased matching learning using elastic netAugmented Inverse Probability Weighting (AIPW) GRF","code":"\nquiet <- function(x) {\n  sink(tempfile())\n  on.exit(sink())\n  invisible(force(x))\n}\n\n# difference in means\ndiff <- function(data, Y, treat) {\n  fml <- as.formula(paste(Y, \"~\", treat))\n  out <- summary(lm_robust(fml, data = data, se_type = \"stata\"))$coefficients[treat, c(1, 2, 5, 6)]\n  return(out) # extract coef, se, ci.lower, ci.upper\n}\n\n\n# regression adjustment\nreg <- function(data, Y, treat, covar) {\n  fml <- as.formula(paste(Y, \"~\", treat, \"+\", paste(covar, collapse = \" + \")))\n  out <- summary(lm_robust(fml, data = data, se_type = \"stata\"))$coefficients[treat, c(1, 2, 5, 6)]\n  # extract coef, se, ci.lower, ci.upper\n  return(out)\n}\n\n# matching\n#library(Matching)\nmatching <- function(data, Y, treat, covar) {\n  m.out <- Match(Y = data[, Y], Tr = data[, treat], X = data[, covar], Z = data[, covar],\n                 estimand = \"ATT\", M = 5, replace = TRUE, ties = TRUE, BiasAdjust = TRUE)\n  out <- c(m.out$est[1], m.out$se[1], m.out$est[1] - 1.96 * m.out$se[1],\n           m.out$est[1] + 1.96 * m.out$se[1])\n  return(out)\n}\n\npsm <- function(data, Y, treat, covar) {\n  ps <- probability_forest(X = data[, covar],\n                           Y = as.factor(data[,treat]), seed = 1234, num.trees = 4000)$predictions[,2]\n  m.out <- Match(Y = data[, Y], Tr = data[, treat], X = matrix(ps, nrow(data), 1),\n                 estimand = \"ATT\", M = 1, replace = FALSE, ties = FALSE, BiasAdjust = FALSE)\n  if (is.null(m.out$se)==FALSE) {\n    se <- m.out$se[1]\n  } else {\n    se <- m.out$se.standard[1]\n  }\n  out <- c(m.out$est[1], se, m.out$est[1] - 1.96 * se,\n           m.out$est[1] + 1.96 * se)\n  return(out)\n}\n\n\n# OM (reg)\nom.reg <- function(data, Y, treat, covar) {\n  tr <- which(data[, treat] == 1)\n  co <- which(data[, treat] == 0)\n  fml <- as.formula(paste(Y, \"~\", paste(covar, collapse = \" + \")))\n  out.co <- lm(fml, data = data[co, ])\n  Y.tr.hat <- predict(out.co, newdata = data[tr, covar, drop = FALSE])\n  newdata <- cbind.data.frame(Y = c(data[tr, Y], Y.tr.hat), treat = rep(c(1, 0), each = length(tr)))\n  out <- summary(lm_robust(Y ~ treat, data = newdata, se_type = \"stata\"))$coefficients[\"treat\", c(1, 2, 5, 6)]\n  return(out)\n}\n\n# OM (grf)\n#library(grf)\nom.grf <- function(data, Y, treat, covar) {\n  tr <- which(data[, treat] == 1)\n  co <- which(data[, treat] == 0)\n  out.co <- regression_forest(X = data[co, covar, drop = FALSE], Y = as.vector(data[co, Y]) )\n  Y.tr.hat <- as.vector(unlist(predict(out.co, newdata = data[tr, covar, drop = FALSE])))\n  newdata <- cbind.data.frame(Y = c(data[tr, Y], Y.tr.hat), treat = rep(c(1, 0), each = length(tr)))\n  out <- summary(lm_robust(Y ~ treat, data = newdata, se_type = \"stata\"))$coefficients[\"treat\", c(1, 2, 5, 6)]\n  return(out)\n}\n\n\n# IPW\nipw <- function(data, Y, treat, covar) {\n  ps <- probability_forest(X = data[, covar, drop = FALSE], Y = as.factor(data[, treat]), seed = 1234)$predictions[,2]\n  fml <- as.formula(paste(Y, \"~\", treat))\n  weights <- rep(1, nrow(data))\n  co <- which(data[, treat] == 0)\n  weights[co] <- ps[co]/(1-ps[co])\n  out <- summary(lm_robust(fml, data = data, weights = weights, se_type = \"stata\"))$coefficients[treat, c(1, 2, 5, 6)]\n  # extract coef, se, ci.lower, ci.upper\n  return(out)\n}\n\n# CBPS\n#library(\"CBPS\")\ncbps <- function(data, Y, treat, covar) {\n  fml <- as.formula(paste(treat, \"~\", paste(covar, collapse = \" + \")))\n  ps <- quiet(CBPS(fml, data = data, standardize = TRUE)$fitted.values)\n  fml <- as.formula(paste(Y, \"~\", treat))\n  weights <- rep(1, nrow(data))\n  co <- which(data[, treat] == 0)\n  weights[co] <- ps[co]/(1-ps[co])\n  out <- summary(lm_robust(fml, data = data, weights = weights, se_type = \"stata\"))$coefficients[treat, c(1, 2, 5, 6)]\n  return(out)\n}\n\n# ebal\n#library(hbal)\nebal <- function(data, Y, treat, covar) {\n  ebal.out <- hbal::hbal(Y = Y, Treat = treat, X = covar,  data = data, expand.degree = 1)\n  out <- hbal::att(ebal.out, dr = FALSE)[1, c(1, 2, 5, 6)]\n  return(out)\n}\n\n# hbal\n# hbal <- function(data, Y, treat, covar) {\n#   hbal.out <- hbal::hbal(Y = Y, Treat = treat, X = covar,  data = data, expand.degree = 2, # cv = TRUE)\n#   out <- hbal::att(hbal.out, dr = FALSE)[1, c(1, 2, 5, 6)]\n#   return(out)\n# }\n\n\n# AIPW\naipw <- function(data, Y, treat, covar) {\n  #library(\"grf\")\n  for (var in c(Y, treat, covar)) {\n    data[, var] <- as.vector(data[, var])\n  }\n  c.forest <- causal_forest(X = data[, covar, drop = FALSE], Y = data[, Y],\n                            W = data[, treat], seed = 1234)\n  att <- average_treatment_effect(c.forest, target.sample = \"treated\", method = \"AIPW\")\n  att <- c(att, att[1] - 1.96 * att[2], att[1] + 1.96 * att[2])\n  return(att)\n}\n\naipw.match <- function(data, Y, treat, covar) {\n  # match on ps\n  ps <- probability_forest(X = data[, covar], Y = as.factor(data[, treat]), seed = 1234)$predictions[,2]\n  m.out <- Match(Y = data[, Y], Tr = data[, treat], X = ps,\n                 estimand = \"ATT\", M = 1, replace = FALSE, ties = FALSE, BiasAdjust = FALSE)\n  mb <- quiet(MatchBalance(treat ~ ps, data = data, match.out = m.out, nboots= 0))\n  ks <- mb$AfterMatching[[1]]$ks$ks$statistic\n  s <- data[c(m.out$index.treated, m.out$index.control), ]\n  out <- aipw(s, Y, treat, covar)\n  #return(out)\n  return(c(out, ks))\n}\n\n### This script checks for robustness by estimating original model\n### using double/debiased machine learning using DoubleML package\ndml <-function(data, Y = NULL, treat = NULL, covar = NULL, clust_var = NULL, ml_l = lrn(\"regr.lm\"), ml_m = lrn(\"regr.lm\")){\n\n  if(is.null(covar)){\n    stop(\"No controls in specification.\")\n  }\n\n  #require(DoubleML)\n  #require(mlr3learners)\n  #require(fixest)\n  #require(ggplot2)\n\n  if(is.null(clust_var) == TRUE){\n\n    dat = data[,c(Y,treat,covar)]\n    dat = na.omit(dat)\n\n    dml_dat = DoubleMLData$new(dat,\n                               y_col = Y,\n                               d_cols = treat,\n                               use_other_treat_as_covariate = FALSE,\n                               x_cols = covar)\n\n  }else{\n\n    dat = data[,c(Y, treat, covar, clust_var)]\n    dat[,clust_var] = as.numeric(factor(dat[,clust_var]))\n    dat = dat[is.na(dat[,Y]) == FALSE,]\n    dat = dat[is.na(dat[,D]) == FALSE,]\n    features = data.frame(model.matrix(formula(paste(c('~ 1',treat,covar), collapse=\"+\")), dat))\n    dat = cbind(dat[,c(Y,clust_var)],features)\n\n    dml_dat = DoubleMLClusterData$new(dat,\n                                      y_col = Y,\n                                      d_cols = treat,\n                                      cluster_cols = clust_var,\n                                      use_other_treat_as_covariate = FALSE,\n                                      x_cols = covar)\n  }\n\n  # Set active treatment treatment\n  dml_dat$set_data_model(treat)\n\n  # Estimate with DML\n  set.seed(pi)\n  dml_mod = DoubleMLPLR$new(dml_dat, ml_l=ml_l, ml_m=ml_m)\n  quiet(dml_mod$fit())\n  out = c(dml_mod$coef[treat], dml_mod$se[treat], dml_mod$confint()[treat,])\n\n  return(out)\n\n}\n\n# execute all estimators\n## estimate all\nestimate_all <- function(data, Y, treat, covar, \n    methods = c(\"diff\", \"reg\", \"om.reg\", \"om.grf\",\n      \"matching\", \"psm\", \"ipw\", \"cbps\", \"ebal\", \n      \"dml\", \"aipw_grf\")) {\n  \n  results <- as.data.frame(matrix(NA, length(methods), 4))\n  rownames(results) <- methods\n  colnames(results) <- c(\"Estimate\", \"SE\", \"CI_lower\", \"CI_upper\")\n  m <- 1\n  if (\"diff\" %in% methods) {\n    results[m, ] <- diff(data, Y, treat) \n    m <- m + 1\n  }\n  if (\"reg\" %in% methods) {\n    results[m, ] <- reg(data, Y, treat, covar) \n    m <- m + 1\n  }\n  if (\"om.reg\" %in% methods) {\n    results[m, ] <- om.reg(data, Y, treat, covar) \n    m <- m + 1\n  }\n  if (\"om.grf\" %in% methods) {\n    results[m, ] <- om.grf(data, Y, treat, covar) \n    m <- m + 1\n  } \n  if (\"matching\" %in% methods) {\n    results[m, ] <- matching(data, Y, treat, covar) \n    m <- m + 1\n  }\n  if (\"psm\" %in% methods) {\n    results[m, ] <- psm(data, Y, treat, covar) \n    m <- m + 1\n  }  \n  if (\"ipw\" %in% methods) {\n    results[m, ] <- ipw(data, Y, treat, covar) \n    m <- m + 1\n  }\n  if (\"cbps\" %in% methods) {\n    results[m, ] <- cbps(data, Y, treat, covar) \n    m <- m + 1\n  }\n  if (\"ebal\" %in% methods) {\n    results[m, ] <- quiet(ebal(data, Y, treat, covar))\n    m <- m + 1\n  }\n  # if (\"hbal\" %in% methods) {\n  #   results[m, ] <- quiet(hbal(data, Y, treat, covar))\n  #   m <- m + 1\n  # }\n  if (\"dml\" %in% methods) {\n    results[m, ] <-dml(data, Y, treat, covar) \n    m <- m + 1\n  }\n  if (\"aipw_grf\" %in% methods) {\n    results[m, ] <- aipw(data, Y, treat, covar) \n    m <- m + 1\n  }\n  return(results)\n}"},{"path":"getting-started.html","id":"function-calls","chapter":"1 Getting Started","heading":"1.2.4.1 Function calls","text":"quiet: Suppresses output function call.diff: Difference means estimator. runs linear regression\nadjusting robust standard errors returns coefficient,\nstandard error, confidence interval treatment variable.reg: Regression adjustment. Similar diff includes\nadditional covariates regression model.matching: Propensity score matching using Matching package. \naligns treated units control units based covariates \nreturns estimated ATT confidence interval.psm: Propensity score matching using probability forest,\nfollowed matching estimation ATT.om.reg: Outcome modeling using regression. predicts outcome\ntreated units based model fitted control\nunits, estimates ATT.om.grf: Outcome modeling using generalized random forests, noted\nGRF.ipw: Inverse probability weighting,denoted IPW. weights\nobservations inverse estimated propensity scores \ncalculates treatment effect weighted regression.cbps: Covariate balancing propensity score, noted CBPS. \nestimates propensity scores achieve balance covariates across\ngroups.ebal: Entropy balancing. reweights data balance \ncovariate distributions.hbal: Hierarchical balancing. extension ebal \ncomplex balancing methods.aipw: Augmented inverse probability weighting, noted AIPW.aipw.match: Combines matching propensity scores AIPW.dml: Double machine learning. uses machine learning algorithms\ncontrol confounders estimating treatment effects.plot_coef plots ATT estimates, allowing visual comparison.","code":"\nplot_coef <- function(out, \n    methods = c(\"diff\", \"reg\", \"om.reg\", \"om.grf\", \n    \"matching\", \"psm\", \"ipw\", \"cbps\", \"ebal\", \n        \"dml\", \"aipw_grf\"),\n    labels = c(\"Diff-in-Means\", \"Reg\", \"OM: Reg\", \"OM: GRF\",\n        \"NN\\nMatching\", \"PS\\nMatching\",\n        \"IPW\", \"CBPS\", \"Ebal\", \"DML\\nElasnet\", \"AIPW-GRF\"),\n    main = NULL,\n    ylab = \"Estimate\",\n    band = NULL,\n    line = NULL,\n    grid = TRUE,\n    main.pos = 1,\n    main.line = -2,\n    ylim = NULL\n) {\n  \n  if (is.null(methods) == TRUE) {\n    methods <- rownames(out)\n  }\n  \n  if (is.null(labels) == TRUE) {\n    labels <- methods\n  }\n  \n  # # check\n  # if (is.null(out)==FALSE) {\n  #   if (inherits(out, \"ivDiag\") == FALSE) {stop(\"\\\"out\\\" needs to be a \\\"ltz\\\" object.\")}\n  # }\n  # \n  # # title\n  # if (is.null(main)==TRUE) {\n  #   main <- \"Estimates with 95% CIs\"\n  # }\n  \n  \n  # Data for the plot\n  data <- out\n  rg <- range(data[,c(3,4)], na.rm = TRUE)\n  adj <- rg[2] - rg[1]\n  if (is.null(ylim) == TRUE) {\n    ylim  <- c(min(0, rg[1] - 0.3*adj), max(0, rg[2] + 0.35*adj))\n  }\n  adj2 <- ylim[2] - ylim[1] \n  \n  # Set up the plot\n  ncoefs <- length(methods)\n  par(mar = c(2.5, 4, 1, 2))\n  plot(1: ncoefs, data[, 1], xlim = c(0.5, ncoefs + 0.5), ylim = ylim,\n       ylab = \"\", xlab = \"\", main = \"\", \n       axes = FALSE, xaxt = \"n\", yaxt = \"n\", type = \"n\")\n  axis(1, at = 1: ncoefs, labels =  labels, las = 1, cex.axis = 0.8)\n  axis(2, cex.axis = 0.7)\n  mtext(main, main.pos, line = main.line, cex = 1.5)\n  mtext(ylab, 2, line = 2.5)\n  if (is.null(band) == FALSE) {\n    rect(-0.5, band[1], ncoefs + 1, band[2], col = \"#ff000030\", border = \"white\") # label at bottom\n  }\n  if (is.null(line) == FALSE) {\n    abline(h = line, col = \"red\", lty = 2)\n  }\n  if (grid == TRUE) {\n    abline(h = axTicks(2), lty = \"dotted\", col = \"gray50\")\n    abline(v = c(0.5, c(1: ncoefs) + 0.5), lty = \"dotted\", col = \"gray50\") # horizontal grid\n  }\n  abline(h = 0, col = \"red\", lwd = 2, lty = \"solid\")\n  segments(y0 = data[, 3], x0 = c(1: ncoefs), y1 = data[, 4], x1 = c(1: ncoefs), lwd = 2) #CI\n  points(1: ncoefs, data[, 1], pch = 16, col = 1, cex = 1.2) #point coefs\n  box()\n}"},{"path":"getting-started.html","id":"catt-and-plot_catt","chapter":"1 Getting Started","heading":"1.2.5 catt and plot_catt","text":"functions aim estimate visualize Conditional Average\nTreatment Effect Treated (CATT). using robust standard errors\n( se_type = “stata”), aim obtain reliable standard errors even\npresence heteroskedasticity violations \nclassical linear regression assumptions.catt estimates CATT using causal forests.plot_catt plots CATT density ATT estimates, allowing \nvisual comparison.","code":"\ncatt <- function(data, Y, treat, covar){\n  tau.forest <- causal_forest(X = data[, covar], Y = data[, Y],\n                              W = data[, treat], num.trees = 4000)\n  tau0 <- average_treatment_effect(tau.forest,\n                                   target.sample = \"treated\", method = \"AIPW\")\n  tau <- tau.forest$predictions\n  tau.tr <- tau[which(data[, treat]==1)]\n  return(list(catt = tau.tr, att = tau0))\n}\nplot_catt <- function(catt1, catt2, att1, att2,\n                      xlab = NULL, ylab = NULL, main = NULL, axes.range = NULL,\n                      file = NULL, width = 7, height = 7) {\n\n  if (is.null(axes.range)==TRUE) {\n    axes.range <- range(c(catt1,catt2))\n  }\n  drange <- axes.range[2] - axes.range[1]\n  axes.range <- axes.range + c(-0.1, 0.1) * drange\n  den1 <- density(catt1)\n  den2 <- density(catt2)\n  max.den <- max(c(den1$y, den2$y))\n  adj <- drange * 0.15 / max.den\n  if (!is.null(file)) {\n    pdf(file = file, width = width, height = height)\n    par(mar = c(4, 4, 3, 2))\n  }\n  plot(1, xlim = axes.range, ylim = axes.range, type = \"n\",\n       xlab = xlab, ylab = ylab, main = main)\n  abline(h = 0, v = 0, col = \"gray\", lty = 3)\n  abline(0, 1, col = \"red\", lty = 2)\n  y1 <- adj * den1$y + axes.range[1] - drange*0.03\n  polygon(den1$x,  y1, col=\"#AAAAAA50\", border = NA)\n  lines(den1$x, y1, col = \"gray\", lwd = 1)\n  y2 <- adj * den2$y + axes.range[1] - drange*0.03\n  polygon(y2, den2$x, col=\"#AAAAAA50\", border = NA)\n  lines(y2, den2$x, col = \"gray\", lwd = 1)\n  points(catt1, catt2, cex = 0.5, col = \"#AAAAAA80\", pch = 16)\n  points(catt1, catt2, cex = 0.5, col = \"#777777\", pch = 1, lwd = 0.5)\n  if (is.null(att1) == FALSE) {\n    points(att1, att2, cex = 2, col = 2, pch = 3, lwd = 2)\n  }\n  box()\n  if (!is.null(file)) {graphics.off()}\n}"},{"path":"getting-started.html","id":"function-calls-1","chapter":"1 Getting Started","heading":"1.2.5.1 Function calls","text":"causal_forest: catt function begins training causal\nforest model using causal_forest function grf\npackage.average_treatment_effect: causal forest trained, \nfunction estimates ATT using doubly-robust AIPW method.","code":""},{"path":"getting-started.html","id":"est_qte-plot_qte","chapter":"1 Getting Started","heading":"1.2.6 est_qte & plot_qte","text":"est_qte function estimates Quantile Treatment Effect \nTreated (QTET) using doubly robust methods. effect \ndifference particular quantile outcome distribution\ntreated untreated units.plot_qte function visualizes QTET estimates.Argumentsps: boolean argument; set TRUE, propensity scores used\nestimation.probs: sequence probabilities quantile\ntreatment effects estimated.cores: Number cores use parallel computation, \nspeeds process.mod, mod2, bm: Within function QTET estimation, \nmod parameter mandatory, whereas mod2 bm optional\nmay included comparative analysis.","code":"\nest_qte <- function(Y, treat, covar, data, ps = TRUE,\n                    probs = seq(0.05,0.95,0.05), cores = 20,\n                    ylim = NULL) {\n  # Set up\n  if (is.null(covar)) {\n    formla <- as.formula(paste(Y, \"~\", treat))\n  } else {\n    formla <- as.formula(paste(Y, \"~\", treat, \"+\", paste(covar, collapse = \"+\")))\n  }\n  if (is.null(covar) | ps == FALSE) {\n    mod <- ci.qtet(formla, xformla = NULL, data = data,\n                   probs = probs, se = TRUE, iters = 1000, pl = TRUE, cores = cores)\n  } else {\n    xformla <- as.formula(paste(\"~\", paste(covar, collapse = \"+\")))\n    mod <- ci.qtet(formla, xformla = xformla, data = data,\n                   probs = probs, se = TRUE, iters = 1000, pl = TRUE, cores = cores)\n  }\n  return(mod)\n}\nplot_qte <- function(mod, mod2 = NULL, bm = NULL, main= \"\", ylim = NULL,\n                     col = NULL) {\n  # ylim\n  if (is.null(ylim)) {\n    ylim <- range(c(mod$qte.lower, mod$qte.upper))\n  }\n  # Plot\n  par(mar = c(3, 3, 1, 1))\n  plot(1, type = \"n\", xlab = \"\", ylab = \"\",\n       xlim = c(0, 1), ylim = ylim, axes = FALSE)\n  box(); axis(1, at = seq(0.1, 0.9, 0.2)); axis(2)\n  mtext(\"QTET\", 2, line = 2)\n  mtext(\"Quantile\", 1, line = 2)\n  abline(h = 0, lty = 1, lwd = 2, col = \"gray\")\n  title(main, line = -1.5)\n  # model 2\n  if (is.null(mod2) == FALSE) {\n    polygon(c(mod2$probs, rev(mod2$probs)), c(mod2$qte.lower, rev(mod2$qte.upper)),\n            col = \"#FC94AF50\", border = NA)\n  }\n  # benchmark\n  if (is.null(bm) == FALSE) {\n    polygon(c(bm$probs, rev(bm$probs)), c(bm$qte.lower, rev(bm$qte.upper)),\n            col = \"#ADD8E680\", border = NA)\n  }\n  # main\n  if (is.null(col) == TRUE) {\n    col1 <- \"gray30\"\n      col2 <- \"#AAAAAA90\"\n  } else {\n    col1 <- col[1]\n    col2 <- col[2]\n  }\n  polygon(c(mod$probs, rev(mod$probs)), c(mod$qte.lower, rev(mod$qte.upper)),\n          col = col2, border = NA)\n  if (is.null(mod2) == FALSE) {\n    lines(mod2$probs, mod2$qte, col = \"#DC143C80\", lwd = 2)\n    points(mod2$probs, mod2$qte, col = \"#DC143C\", pch = 17, cex = 0.8)\n  }\n  if (is.null(bm) == FALSE) {\n    lines(bm$probs, bm$qte, col = 4, lwd = 2)\n    points(bm$probs, bm$qte, col = 4, pch = 16)\n  }\n  lines(mod$probs, mod$qte, col = col1, lwd = 2)\n  lines(mod$probs, mod$qte.lower, col = col1, lty = 3, lwd = 1.5)\n  lines(mod$probs, mod$qte.upper, col = col1, lty = 3, lwd = 1.5)\n  points(mod$probs, mod$qte, col = col1, pch = 16)\n}"},{"path":"getting-started.html","id":"sens_ana","chapter":"1 Getting Started","heading":"1.2.7 sens_ana","text":"sens_ana function conducts sensitivity analysis estimated\ntreatment effect assess susceptible findings potential\nunobserved confounding.","code":"\nsens_ana <- function(data, Y, treat, covar, bm = NULL, kd = 1)\n{\n  p.forest <- probability_forest(X = data[, covar],\n                                 Y = as.factor(data[, treat]), seed = 1234, num.trees = 4000)\n  data$ps_sens <- p.forest$predictions[,2]\n  data <- subset(data, ps_sens > 0.1 & ps_sens < 0.9)\n  fml <- as.formula(paste(Y, \"~\", treat, \"+\", paste(covar, collapse = \"+\")))\n  mod <- lm(fml, data = data)\n  sens <- sensemakr(model = mod, treatment = treat, benchmark_covariates = bm, kd = kd, sensitivity.of = \"t-value\")\n  plot(sens)\n}"},{"path":"getting-started.html","id":"function-calls-2","chapter":"1 Getting Started","heading":"1.2.7.1 Function calls","text":"probability_forest: probability forest trained \ncovariates estimate propensity score using probability \nunit receiving treatment given observed covariates.sensemakr: function sensemakr package utilizes\nsensitivity analysis linear model treatment\nvariable, optional benchmark covariates, kd multiplier \nspecifies range sensitivity analysis terms \nproportion treatment effect due omitted\nvariable.","code":""},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"lalonde-dehejia-wahba-ldw-data","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2 LaLonde-Dehejia-Wahba (LDW) Data","text":"LaLonde (1986) assessed impact National Supported Work Demonstration (NSW) program female male participants, females drawn Aid Families Dependent Children (AFDC) program males three target groups: ex-drug addicts, ex-criminal offenders, high-school dropouts. LaLonde used two primary data sources:CPS-SSA-1, Westat’s Matched Current Population Survey–Social Security Administration File individuals 55 matching specific criteria.PSID-1, Panel Study Income Dynamics household heads 55 specific years retired 1975, adjusted factors like employment status poverty level, resulting four additional comparison groups.Dehejia Wahba (1999) subsampled 62% LaLonde’s original dataset focusing 1974 earnings unemployment status male participants. Since construction relies solely pretreatment information like month assignment employment history ensure treatment assignment remains orthogonal pretreatment variables, Dehejia Wahba argue refined dataset, LaLonde-Dehejia-Wahba (LDW) data, valid experimental sample.Built upon LDW data, analysis examines three samples: (1) LDW-Experimental, 185 treated 280 control participants experimental data; (2) LDW-CPS1, featuring treated individuals alongside 15,992 controls CPS-SSA-1; (3) LDW-PSID1, including treated participants 2,490 controls PSID-1. , apply set statistical tools analyze original male sample LaLonde (1986) section 4 additional demonstration.","code":"\nload(\"data/lalonde.RData\")\nldw_co$treat <- 1\nldw_cps.plus <- rbind.data.frame(ldw_cps, ldw_co)\nldw_psid.plus <- rbind.data.frame(ldw_psid, ldw_co)\n\n# define variables\nY <- \"re78\"\ntreat <- \"treat\"\ncovar <- c(\"age\", \"education\", \"black\", \"hispanic\", \"married\",\n           \"nodegree\", \"re74\", \"re75\", \"u74\", \"u75\")"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"assessing-overlap","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.1 Assessing Overlap","text":"identify average causal effect unconfoundedness, need ensure can estimate average effect every value covariates. Thus, require overlaps treated control units. Using assess_overlap function, can assess overlaps propensity scores. test overlap assumption, plot histograms using log-odds propensity scores, .e., \\(\\log(\\frac{\\hat{e}}{1-\\hat{e}})\\).Ideally, well-balanced experimental design, distributions treated (red) control (gray) groups overlap.","code":""},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"ldw-experimental","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.1.1 LDW-Experimental","text":"\nFigure 2.1: FIGURE1. SubfigureA:LDW-Experimental.\n","code":"\nldw_ps <- assess_overlap(data = ldw, treat = treat, cov = covar)\n#> -1.2254 0.7533904"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"ldw-cps1-and-lwd-psid1","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.1.2 LDW-CPS1 and LWD-PSID1","text":"\nFigure 2.2: FIGURE1. SubfigureB:LDW-CPS1. SubfigureC:LDW-PSID1.\n","code":"\npar(mfrow = c(1,2))\nldw_cps_ps <- assess_overlap(data = ldw_cps, treat = treat, cov = covar)\n#> -16.1181 1.787226\nldw_psid_ps <- assess_overlap(data = ldw_psid, treat = treat, cov = covar)\n#> -16.1181 3.824234"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"overlap-in-original-ldw-samples","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.1.3 Overlap in Original LDW Samples","text":"expected, LDW-Experimental shows almost perfect overlap. However, observational samples show poor overlaps. notably, propensity scores many treated units lie within support controls’ propensity scores, substantial proportion control units possess extremely low log-odds.","code":""},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"trimming-to-improve-overlap","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.2 Trimming to Improve Overlap","text":"Based LDW-CPS1 LWD-PSID1, construct two trimmed samples improve overlap. Trimming involves two steps.First, merge experimental controls LDW-Experimental LDW-CPS1 (LDW-PSID1) enhance control group units. , can estimate unit’s propensity included experiment using GRF. purpose trimming remove units dissimilar group ensure better balance treated control groups. can use trim trim data.LDW-CPS1, threshold set 0.9, meaning unit propensity score higher 0.9 excluded.LDW-PSID1, threshold lower, 0.8, indicating stricter criterion inclusion analysis.Second, using trimmed data set covariates, reestimate propensity scores GRF; time, experimental controls excluded.employ 1:1 matching based reestimated propensity scores trim nonexperimental controls.","code":"\n# re-estimate each unit’s propensity by merged data\nldw_cps_ps <- assess_overlap(data = ldw_cps.plus, treat = treat, cov = covar)\n#> -16.1181 3.542706\nldw_psid_ps <- assess_overlap(data = ldw_psid.plus, treat = treat, cov = covar)\n#> -16.1181 7.102692\n\ntrim <- function(data, ps = \"ps_assoverlap\", threshold = 0.9) {\n  sub <- data[which(data[, ps] < threshold), ]\n  return(sub)\n}\n\nldw_cps_trim <- trim(ldw_cps_ps, threshold = 0.9)\nldw_psid_trim <- trim(ldw_psid_ps, threshold = 0.8)\n# cps data\n# excluding the experimental controls\nldw_cps_trim_match <- subset(ldw_cps_trim, sample %in% c(1,3) & ps_assoverlap)\n# re-estimate propensity scores and employ 1:1 matching\nldw_cps_trim_match <- psmatch(data = ldw_cps_trim_match, Y = \"re78\", treat = \"treat\", cov = covar)\n\n# psid data\n# excluding the experimental controls\nldw_psid_trim_match <- subset(ldw_psid_trim, sample %in% c(1,4) & ps_assoverlap)\n# re-estimate propensity scores and employ 1:1 matching\nldw_psid_trim_match <- psmatch(data = ldw_psid_trim_match, Y = \"re78\", treat = \"treat\", cov = covar)\n# We conduct this procedure to trim all samples simultaneously to improve overlap in the final samples.\n\n#cps\nldw_trim_cps <- subset(ldw_cps_trim, sample %in% c(1,2) & ps_assoverlap <= 0.9)\nldw_trim_cps$treat[which(ldw_trim_cps$sample == 2)] <- 0\n#psid\nldw_trim_psid <- subset(ldw_psid_trim, sample %in% c(1,2) & ps_assoverlap <= 0.8)\nldw_trim_psid$treat[which(ldw_trim_psid$sample == 2)] <- 0"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"reassessing-overlap","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.3 Reassessing Overlap","text":"shown following figures, overlap improves significantly samples post-trimming, though comes cost reduced sample sizes.\nFigure 2.3: FIGURE1. SubfigureD:TrimmedLDW-CPS1. SubfigureE:TrimmedLDW-PSID1.\n","code":"\npar(mfrow = c(1,2))\n# cps data\nldw_cps_trim_match_ps <- assess_overlap(data = ldw_cps_trim_match, treat = treat, cov = covar, xlim = c(-3,3))\n\n# psid data\nldw_psid_trim_match_ps <- assess_overlap(data = ldw_psid_trim_match, treat = treat, cov = covar, xlim = c(-3,3))"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"estimating-the-att","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.4 Estimating the ATT","text":"Next, estimate ATT using original LDW observational samples newly constructed trimmed samples. apply variety estimators, including simple difference--means, regression, OaxacaBlinderestimator, GRF outcome model, 1: 5 nearest neighbor matching bias correction, IPW propensity scores estimated GRF, covariate balancing propensity score (CBPS), entropy balancing, double/debiased matching learning using elastic net, AIPW implemented via GRF. estimators use set ten covariates .achieve comprehensive analysis ATT, can use estimate_all function.\nFigure 2.4: FIGURE 3. ATT Estimates Given Unconfoundedness: LDW Samples\nfigures show ATT estimates 95% confidence intervals using four different samples: LDW-CPS1, LDW-PSID1, Trimmed LDW-CPS1, Trimmed LDW-PSID1.shown Figure 3(), using LDW-CPS1, estimators, except difference -means, produce positive estimates, although noticeable variations among . Nearest neighbor matching outperforms estimators, aligning closely experimental benchmark $1,794. Notably, CBPS, entropy balancing, AIPW-GRF also produce results close benchmark. Despite numerical differences, estimates, except difference--means, statistically distinguished one another. Figure 3(B) shows estimates based LDW-PSID1 exhibit greater variations. Setting aside difference--means, estimates span $4 $2,420. Among , AIPW-GRF estimator produces estimate closest experimental benchmark.Figure 3(C) (D) show using trimmed data improved overlap, estimates produced various estimators substantially stable.ATT results presented table :Table 2.1:  Table A1 Supplementary Materials (SM)Columns 1 2 report estimates LDW-CPS1 LDW-PSID1, respectively, columns 3 4 report estimates trimmed samples improved overlap. Robust standard errors parentheses. Improved overlap trimmed samples generally leads estimates higher consistency benchmark. trimmed samples often show increased standard errors, expected trimming reduces sample size, thus increasing variance.shown column 1, using LDW-CPS1, estimators, except difference--means, produce estimates exceeding $1,000, although noticeable variations among . Nearest neighbor matching outperforms estimators, aligning closely experimental benchmark $1,794. Notably, propensity score matching, entropy balancing, AIPW-GRF also produce results close benchmark. standard errors estimates large. result, despite numerical differences, estimates, except difference--means, statistically distinguished one another. Column 2 shows estimates based LDW-PSID1 exhibit greater variations.findings suggest improved overlap based observed covariates can reduce model dependency estimate variability across different estimators, guarantee consistency without validating unconfoundedness.","code":"\nload(\"data/trimmed.RData\")\n\n# experimental\nout1 <- estimate_all(data = ldw, Y = \"re78\", treat = \"treat\", cov = covar)\nout2 <- estimate_all(ldw_trim_cps, \"re78\", \"treat\", covar)\nout3 <- estimate_all(ldw_trim_psid, \"re78\", \"treat\", covar)\n# nonexperimental\nout4 <- estimate_all(ldw_cps, \"re78\", \"treat\", covar)\nout5 <- estimate_all(ldw_psid, \"re78\", \"treat\", covar)\n#> Warning in average_treatment_effect(c.forest, target.sample\n#> = \"treated\", : Estimated treatment propensities take values\n#> between 0.003 and 0.988 and in particular get very close to\n#> 0 and 1. In this case, using `target.sample=overlap`, or\n#> filtering data as in Crump, Hotz, Imbens, and Mitnik\n#> (Biometrika, 2009) may be helpful.\nout6 <- estimate_all(ldw_cps_trim, \"re78\", \"treat\", covar)\nout7 <- estimate_all(ldw_psid_trim, \"re78\", \"treat\", covar)\npar(mfrow = c(4,1))\nband <- out1[1, 3:4]\nest <- out1[1, 1]\nplot_coef(out4, band = band, line = est, ylim = c(-15500, 5500), main = \"(A) LDW-CPS1\")\n\nband <- out1[1, 3:4]\nest <- out1[1, 1]\nplot_coef(out5, band = band, line = est, ylim = c(-15500, 5500), main = \"(B) LDW-PSID1\")\n\nband <- out2[1, 3:4]\nest <- out2[1, 1]\nplot_coef(out6, band = band, line = est, ylim = c(-15500, 5500), main = \"(C) Trimmed LDW-CPS1\")\n\nband <- out3[1, 3:4]\nest <- out3[1, 1]\nplot_coef(out7, band = band, line = est, ylim = c(-15500, 5500), main = \"(D) Trimmed LDW-PSID1\")\n# print the result\na <- list(out4, out5, out6, out7)\nn <- nrow(out1)\nsav <- matrix(\"\", n+1, length(a)*3-1)\nfor (j in 1:length(a)) {\n    out <- a[[j]]\n    n <- nrow(out)\n    for (i in 2:(nrow(out)+1)) {\n        sav[i, j*3-2] <- sprintf(\"%.2f\", out[i-1, 1])\n        sav[i, j*3-1] <- paste0(\"(\", sprintf(\"%.2f\", out[i-1, 2]), \")\")\n    }\n}\nsav[1, 1] <- sprintf(\"%.2f\", out1[1, 1])\nsav[1, 2] <- paste0(\"(\", sprintf(\"%.2f\", out1[1, 2]), \")\")\nsav[1, 4] <- sprintf(\"%.2f\", out1[1, 1])\nsav[1, 5] <- paste0(\"(\", sprintf(\"%.2f\", out1[1, 2]), \")\")\nsav[1, 7] <- sprintf(\"%.2f\", out2[1, 1])\nsav[1, 8] <- paste0(\"(\", sprintf(\"%.2f\", out2[1, 2]), \")\")\nsav[1, 10] <- sprintf(\"%.2f\", out3[1, 1])\nsav[1, 11] <- paste0(\"(\", sprintf(\"%.2f\", out3[1, 2]), \")\")\ncolnames(sav) <- c(\"LDW-CPS1\", \"\", \"\", \"LDW-PSID1\", \"\", \"\", \"LDW-CPS1 (PS Trimmed) \", \"\", \"\", \"LDW-PSID1 (PS Trimmed)\", \"\")\nrownames(sav) <- c(\"Experimental Benchmark\", \"Difference-in-Means\", \"Regression\", \" Oaxaca Blinder\", \"GRF\", \"NN Matching\", \"PS Matching\", \"IPW\", \"CBPS\", \"Entropy Balancing\", \"DML-ElasticNet\", \"AIPW-GRF\")\nsav %>% knitr::kable(booktabs=TRUE, caption = \" Table A1 in the Supplementary Materials (SM)\")"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"alternative-estimands-catt-and-qtet","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.5 Alternative Estimands: CATT and QTET","text":"","code":""},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"conditional-average-treatment-effect-on-the-treated-catt","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.5.1 Conditional Average Treatment Effect on the Treated (CATT)","text":"Exploring causal estimates alternative estimands, heterogeneous treatment effects quantile treatment effects, can help assess unconfoundedness.CATT can reveal treatment effect varies across different subgroups defined covariates. Using original LDW data trimmed versions, estimate CATT using causal forest AIPW-GRF. can use wrapper function catt estimate CATT., can use plot_catt visualize result. following figures, plot estimated CATT observational data covariate values treated unit corresponding experimental benchmarks. gray dot represents pair CATT estimates, red cross depicts pair estimated ATTs.\nFigure 2.5: FIGURE 4. CATT Estimates using LDW Data: Experimental vs. Nonexperimental\nAlthough AIPW estimator can produce ATT estimates closely aligned experimental benchmark using LDW data, performance revealing true CATT considerably worse. Specifically, LDW-CPS1, CATT estimates span $-5,456.1 $6,997.1, contrasting CATT estimated experimental data ranges $-345.3 $4,148.5. overestimates CATT exceeds ATT underestimates CATT falls ATT. Employing LDW-PSID generates CATT estimates ranging $-8131.2 $4381.0. trimmed LDW-CPS, CATT estimates align closely experimental data. However, using trimmed LDW-PSID, majority CATT estimates negative, suggesting significant biases.","code":"\n# estimate catt\ncatt.ldw <- catt(ldw, Y, treat, covar)\ncatt.cps <- catt(ldw_cps, Y, treat, covar)\ncatt.psid <- catt(ldw_psid, Y, treat, covar)\ncatt.cps.trim <- catt(ldw_cps_trim, Y, treat, covar)\ncatt.psid.trim <- catt(ldw_psid_trim, Y, treat, covar)\n# trimmed experimental data\ncatt.ldw.cps <- catt(ldw_trim_cps, Y, treat, covar)\ncatt.ldw.psid <- catt(ldw_trim_psid, Y, treat, covar)\npar(mfrow = c(2,2))\n# plot catt - \"CATT (Experimental)\" and \"CATT (CPS-Full)\"\ncatt1 <- catt.ldw$catt\natt1 <- catt.ldw$att[1]\ncatt2 <- catt.cps$catt\natt2 <- catt.cps$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (CPS-Full)\",\n          main = \"\", c(-8000, 8000))\n\n# plot catt - \"CATT (Experimental)\" and \"CATT (PSID-Full)\"\ncatt1 <- catt.ldw$catt\natt1 <- catt.ldw$att[1]\ncatt2 <- catt.psid$catt\natt2 <- catt.psid$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (PSID-Full)\",\n    main = \"\", c(-8000, 8000))\n\n# plot catt - \"CATT (Experimental)\" and \"CATT (CPS-Trimmed)\"\ncatt1 <- catt.ldw.cps$catt\natt1 <- catt.ldw.cps$att[1]\ncatt2 <- catt.cps.trim$catt\natt2 <- catt.cps.trim$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (CPS-Trimmed)\",\n    main = \"\", c(-8000, 8000))\n\n# plot catt - \"CATT (Experimental)\" and \"CATT (PSID-Trimmed)\"\ncatt1 <- catt.ldw.psid$catt\natt1 <- catt.ldw.psid$att[1]\ncatt2 <- catt.psid.trim$catt\natt2 <- catt.psid.trim$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (PSID-Trimmed)\",\n    main = \"\", c(-8000, 8000))"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"quantile-treatment-effect-on-the-treated-qtet","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.5.2 Quantile Treatment Effect on the Treated (QTET)","text":"QTET less sensitive outliers can uncover heterogeneity treatment effects may obscured average treatment effect estimates. calculation QTET uses propensity score re-weighting approach proposed Firpo (2007).proceed, can use wrapper function qte.can use plot_qte plot result compare treatment effects estimated trimming based propensity scores.dataset, three lines representing:Experimental (blue line diamond markers): QTET estimates based experimental data, serve benchmark.Unadjusted (pink line triangle markers): QTET estimates observational data without adjustments.Adjusted (black line circle markers): QTET estimates observational data adjusting covariates.\nFigure 2.6: FIGURE 5. Quantile Treatment Effects: Experimental vs. Nonexperimental\nfigures plot QTET estimates using LDW experimental data non-experimental data. QTET estimates either original trimmed LDW-CPS1 data align reasonably well true QTET, although often underpowered. contrast, QTET original trimmed LDW-PSID1 data displays notable biases compared experimental benchmark, close zero.analysis suggests , considering alternative estimands CATT QTET among four observational samples, trimmed LDW-CPS1 yields results consistently aligned closely experimental benchmark.","code":"\n# estimate qte (some of the following lines are not run due to computational limitation)\n\n## experimental\nqte.ldw <- est_qte(Y, treat, NULL, data = ldw)\nqte.ldw.cps <- est_qte(Y, treat, NULL, data = ldw_trim_cps)\nqte.ldw.psid <- est_qte(Y, treat, NULL, data = ldw_trim_psid)\n\n## non-experimental\n#qte.ldw_cps <- est_qte(Y, treat, covar, data = ldw_cps) # adjusted\n#qte.ldw_cps0 <- est_qte(Y, treat, NULL, data = ldw_cps) # unadjusted\nqte.ldw_cps.trim <- est_qte(Y, treat, covar, data = ldw_cps_trim) # adjusted\nqte.ldw_cps.trim0 <- est_qte(Y, treat, NULL, data = ldw_cps_trim) # unadjusted\n#qte.ldw_psid <- est_qte(Y, treat, covar, data = ldw_psid) # adjusted\n#qte.ldw_psid0 <- est_qte(Y, treat, NULL, data = ldw_psid) # unadjusted\nqte.ldw_psid.trim <- est_qte(Y, treat, covar, data = ldw_psid_trim) # adjusted\nqte.ldw_psid.trim0 <- est_qte(Y, treat, NULL, data = ldw_psid_trim) # unadjusted\n# plot qte\n\n#load the data\nload(\"data/qte_ldw.rds\")\n\npar(mfrow = c(2,2))\n# CPS\nplot_qte(qte.ldw_cps, qte.ldw_cps0, qte.ldw, main = \"LDW-CPS\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")\n\n## CPS trimmed\nplot_qte(qte.ldw_cps.trim, qte.ldw_cps.trim0, qte.ldw.cps, main = \"LDW-CPS (Trimmed)\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), \n    lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")\n\n# PSID\nplot_qte(qte.ldw_psid, qte.ldw_psid0, qte.ldw, main = \"LDW-PSID\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), \n    lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")\n\n# PSID trimmed\nplot_qte(qte.ldw_psid.trim, qte.ldw_psid.trim0, qte.ldw.psid, main = \"LDW-PSID (Trimmed)\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), \n    lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"validation-through-placebo-analyses","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.6 Validation through Placebo Analyses","text":"conduct placebo analyses assess plausibility unconfoundedness. , select earnings 1975 (re75) placebo outcome remove re75 u75 set conditioning variables. trimmed samples based 1:1 matching propensity scores estimated via GRF. (Two new trimmed samples created without using re75 u75.) Similarly, can use estimate_all calculate ATT placebo\nanalysis, conditional remaining covariates.\nFigure 2.7: FIGURE 6. Placebo Test: ’75 Earnings Outcome\nFigure 6 presents findings. surprisingly, experimental benchmarks near zero statistically insignificant. However, estimators using nonexperimental data generate large, negative estimates. , trimmed data, estimates stable remain statistically different zero.Placebo ATT results presented table :Table 2.2:  Table A2 Supplementary Materials (SM)first row table shows experimental benchmarks near zero statistically insignificant. placebo analysis, estimators using observational data generate large, negative estimates.Moreover, almost estimators significantly different targeted analysis. Since placebo estimates always large negative estimates former analysis smaller magnitude positive, suggest methods sensitive true effect.","code":"\nY <- \"re75\"\ntreat <- \"treat\"\ncovar <- c(\"age\", \"education\", \"black\", \"hispanic\", \"married\", \"nodegree\", \"re74\", \"u74\")\n\n# experimental\nout1 <- estimate_all(ldw, Y, \"treat\", covar)\nout2 <- estimate_all(ldw_trim_cps_pl, Y, \"treat\", covar)\nout3 <- estimate_all(ldw_trim_psid_pl, Y, \"treat\", covar)\n# no experimental\nout4 <- estimate_all(ldw_cps, Y, \"treat\", covar)\nout5 <- estimate_all(ldw_psid, Y, \"treat\", covar)\nout6 <- estimate_all(ldw_cps_trim_pl, Y, \"treat\", covar)\nout7 <- estimate_all(ldw_psid_trim_pl, Y, \"treat\", covar)\npar(mfrow = c(4,1))\nband <- out1[1, 3:4]\nest <- out1[1, 1]\nylim <- c(-12000, 2000)\nplot_coef(out4, band = band, line = est, ylim = ylim, main = \"(A) LDW-CPS1\")\n\nband <- out1[1, 3:4]\nest <- out1[1, 1]\nplot_coef(out5, band = band, line = est, ylim = ylim, main = \"(B) LDW-PSID1\")\n\nband <- out2[1, 3:4]\nest <- out2[1, 1]\nplot_coef(out6, band = band, line = est, ylim = ylim, main = \"(C) Trimmed LDW-CPS1\")\n\nband <- out3[1, 3:4]\nest <- out3[1, 1]\nplot_coef(out7, band = band, line = est, ylim = ylim, main = \"(D) Trimmed LDW-PSID1\")\n# print the result\na <- list(out4, out5, out6, out7)\nn <- nrow(out1)\nsav <- matrix(\"\", n+1, length(a)*3-1)\nfor (j in 1:length(a)) {\n    out <- a[[j]]\n    n <- nrow(out)\n    for (i in 2:(nrow(out)+1)) {\n        sav[i, j*3-2] <- sprintf(\"%.2f\", out[i-1, 1])\n        sav[i, j*3-1] <- paste0(\"(\", sprintf(\"%.2f\", out[i-1, 2]), \")\")\n    }\n}\nsav[1, 1] <- sprintf(\"%.2f\", out1[1, 1])\nsav[1, 2] <- paste0(\"(\", sprintf(\"%.2f\", out1[1, 2]), \")\")\nsav[1, 4] <- sprintf(\"%.2f\", out1[1, 1])\nsav[1, 5] <- paste0(\"(\", sprintf(\"%.2f\", out1[1, 2]), \")\")\nsav[1, 7] <- sprintf(\"%.2f\", out2[1, 1])\nsav[1, 8] <- paste0(\"(\", sprintf(\"%.2f\", out2[1, 2]), \")\")\nsav[1, 10] <- sprintf(\"%.2f\", out3[1, 1])\nsav[1, 11] <- paste0(\"(\", sprintf(\"%.2f\", out3[1, 2]), \")\")\ncolnames(sav) <- c(\"LDW-CPS1\", \"\", \"\", \"LDW-PSID1\", \"\", \"\", \"LDW-CPS1 (PS Trimmed) \", \"\", \"\", \"LDW-PSID1 (PS Trimmed)\", \"\")\nrownames(sav) <- c(\"Experimental Benchmark\", \"Difference-in-Means\", \"Regression\", \" Oaxaca Blinder\", \"GRF\", \"NN Matching\", \"PS Matching\", \"IPW\", \"CBPS\", \"Entropy Balancing\", \"DML-ElasticNet\", \"AIPW-GRF\")\nsav %>% knitr::kable(booktabs=TRUE, caption = \" Table A2 in the Supplementary Materials (SM)\")"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"sensitivity-analyses","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.7 Sensitivity Analyses","text":"also conduct sensitivity analyses using LDW data, results depicted contour plots . can use sens_ana conduct sensitivity analyses.\nFigure 2.8: FIGURE A3. Sensitivity Analyses Trimmed LDW-CPS1 LDW-PSID1\nanalyses suggest estimated training effect based trimmed LDW-CPS less sensitive potential confounders compared trimmed LDW-PSID. instance, trimmed LDW-CPS, estimate remains positive large even confounder’s correlations treatment outcome triple re75, whereas presence re75 lead negative estimated effect using trimmed LDW-PSID.","code":"\npar(mfrow = c(1,2))\nY <- \"re78\"\ntreat <- \"treat\"\ncovar <- c(\"age\", \"education\", \"black\", \"hispanic\", \"married\", \"nodegree\", \"re74\", \"re75\", \"u74\", \"u75\")\nbm <- c(\"re75\")\n\n# ldw data\n# sens_ana(ldw, Y, treat, covar, bm)\n\n# trimmed LDW-CPS data\nsens_ana(ldw_cps, Y, treat, covar, bm, kd = 1:3)\n\n# trimmed LDW-PSID data\nsens_ana(ldw_psid, Y, treat, covar, bm)"},{"path":"lalonde-dehejia-wahba-ldw-data.html","id":"summary","chapter":"2 LaLonde-Dehejia-Wahba (LDW) Data","heading":"2.8 Summary","text":"reexamining LDW data original Lalonde male sample, offer new insights challenge posed LaLonde. First, agree existing literature ensuring overlap using comparable control units essential credible causal estimates. Second, choice method less critical overlap, methods yield similar results, propensity score remains vital tool assessing overlap integral many estimators. Third, stress importance understanding treatment assignment mechanism need additional tests validate unconfoundedness. LDW dataset somewhat unique many methods approximate experimental benchmark average effects overlap, success mirrored original LaLonde data. However, even LDW data, placebo tests fail substantiate unconfoundedness, sensitivity analysis reveals fragility regression estimate using LDW-PSID1.","code":""},{"path":"original-lalonde-data-male-sample.html","id":"original-lalonde-data-male-sample","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3 Original LaLonde Data (Male Sample)","text":"section, focus original LaLonde data set (loaded NSW ) assess effect treatment (participation job training program) participants’ earnings 1978.","code":""},{"path":"original-lalonde-data-male-sample.html","id":"prepare-the-data","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.1 Prepare the Data","text":"First, dataset split treatment control groups treatment dummy. Note, two pretreatment variables (earnings 1974 employment status 1974) absent sample. thus remove variables re74 (earnings 1974), u74 (unemployment status 1974), tau (treatment effect estimate) CPS-1 PSID-1 datasets match structure treatment dataset.","code":"\nload(\"data/lalonde.RData\")\nload(\"data/trimmed.RData\")\ntreat <- \"treat\"\nnsw_co$treat <- 1\n\n# drop re74, u74, tau from cps1 and psid1\ncps1a <- subset(cps1, select =  -c(re74, u74))\nnsw_cps <- rbind.data.frame(nsw_tr, cps1a)\n\npsid1a <- subset(psid1, select =  -c(re74, u74))\nnsw_psid <- rbind.data.frame(nsw_tr, psid1a)\n\n\nnsw_cps.plus <- rbind.data.frame(nsw_cps, nsw_co)\nnsw_psid.plus <- rbind.data.frame(nsw_psid, nsw_co)"},{"path":"original-lalonde-data-male-sample.html","id":"assessing-overlap-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.2 Assessing Overlap","text":"step, define variables outcome (Y = “re78”), treatment indicator (treat), covariates include age, education, race/ethnicity indicators, marital status, degree status, earnings 1975., assess overlap covariate distributions treated control groups based propensity score via GRF (log odds ratio) LDW-Experimental, LDW-CPS1, LWD-PSID1 data.","code":"\n# define variables\nY <- \"re78\"\ntreat <- \"treat\"\ncovar <- c(\"age\", \"education\", \"black\", \"hispanic\", \"married\", \"nodegree\", \"re75\", \"u75\")"},{"path":"original-lalonde-data-male-sample.html","id":"nsw-experimental","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.2.1 NSW-Experimental","text":"Figure: Experimental (male sample).\nFigure 3.1: FIGUREA5 (SM). SubfigureA:LDW-Experimental.\n","code":"\nnsw_ps <- assess_overlap(data = nsw, treat = treat, cov = covar, xlim = c(-2, 1.5))"},{"path":"original-lalonde-data-male-sample.html","id":"nsw-cps1-and-nsw-psid1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.2.2 NSW-CPS1 and NSW-PSID1","text":"\nFigure 3.2: FIGUREA5 (SM). SubfigureB:NSW-CPS1. SubfigureC:NSW-PSID1.\n","code":"\npar(mfrow = c(1,2))\nnsw_cps_ps <- assess_overlap(data = nsw_cps, treat = treat, cov = covar, xlim = c(-15, 5))\nnsw_psid_ps <- assess_overlap(data = nsw_psid, treat = treat, cov = covar, xlim = c(-15, 5))"},{"path":"original-lalonde-data-male-sample.html","id":"trimming-to-improve-overlap-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.3 Trimming to Improve Overlap","text":"trim data improve overlap covariate distributions removing units poor overlap based propensity score. step aims refine datasets improve later causal inference. trimmed data, can reassess overlap group.Like , start assessing overlaps distributions treated control groups based log-odds derived propensity scores., proceed trimming improve quality causal inference. trimming, expect distributions align closely - treatment control groups comparable according covariates.","code":"\nnsw_cps.plus_ps <- assess_overlap(data = nsw_cps.plus, treat = treat, cov = covar, xlim = c(-15, 5))\nnsw_psid.plus_ps <- assess_overlap(data = nsw_psid.plus, treat = treat, cov = covar, xlim = c(-15, 5))\n#Trim\nnsw_cps_trim <- trim(nsw_cps.plus_ps, threshold = 0.85)\nnsw_psid_trim <- trim(nsw_psid.plus_ps, threshold = 0.85)\n\n# cps data\n# excluding the experimental controls\nnsw_cps_trim_match <- subset(nsw_cps_trim, sample %in% c(0,3) & ps_assoverlap)\n# re-estimate propensity scores and employ 1:1 matching\nnsw_cps_trim_match <- psmatch(data = nsw_cps_trim_match, Y = \"re78\", treat = \"treat\", cov = covar)\n\n# psid data\n# excluding the experimental controls\nnsw_psid_trim_match <- subset(nsw_psid_trim, sample %in% c(0,4) & ps_assoverlap)\n# re-estimate propensity scores and employ 1:1 matching\nnsw_psid_trim_match <- psmatch(data = nsw_psid_trim_match, Y = \"re78\", treat = \"treat\", cov = covar)\nthreshold = 0.85\n#cps\nnsw_trim_cps <- subset(nsw_cps_trim, sample %in% c(0,0.5))\nnsw_trim_cps$treat[which(nsw_trim_cps$sample == 0.5)] <- 0\n#psid\nnsw_trim_psid <- subset(nsw_psid_trim, sample %in% c(0,0.5))\nnsw_trim_psid$treat[which(nsw_trim_psid$sample == 0.5)] <- 0"},{"path":"original-lalonde-data-male-sample.html","id":"reassessing-overlap-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.4 Reassessing Overlap","text":"propensity scores reestimated trimming.plots show good overlaps especially center, indicating improved balance common support treated control groups. -trimming comparison suggests trim effectively removes units less comparable.\nFigure 3.3: FIGUREA5 (SM). SubfigureD:TrimmedLDW-CPS1. SubfigureE:TrimmedLDW-PSID1.\n","code":"\npar(mfrow = c(1,2))\n# cps data\nnsw_cps_trim_match_ps <- assess_overlap(data = nsw_cps_trim_match, treat = treat, cov = covar, xlim = c(-3,3))\n\n# psid data\nnsw_psid_trim_match_ps <- assess_overlap(data = nsw_psid_trim_match, treat = treat, cov = covar, xlim = c(-3,3))"},{"path":"original-lalonde-data-male-sample.html","id":"estimating-the-att-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.5 Estimating the ATT","text":"table presents ATT estimates using original LaLonde male sample, LDW sample subset. Table shows , sufficient overlap, estimators yield estimates within relatively narrow ranges using either CPS-SSA-1 PSID-1 control groups. However, estimates align experimental benchmarks, estimates negative.Table 3.1:  Table A4 Supplementary Materials (SM), ATT Estimates: LaLonde Male SampleThe table lists ATT estimates using different statistical methods assess impact treatment-job training program- earnings 1978 (re78). results shown two control groups (CPS1 PSID1) trimming based propensity scores improve match treated control units. trimming generally moves estimates closer experimental benchmark, estimates derived observational data generally lower (negative) experimental benchmark.","code":"\n# experimental\nout1 <- estimate_all(nsw, \"re78\", \"treat\", covar)\nout2 <- estimate_all(nsw_trim_cps, \"re78\", \"treat\", covar)\nout3 <- estimate_all(nsw_trim_psid, \"re78\", \"treat\", covar)\n# nonexperimental\nout4 <- estimate_all(nsw_cps, \"re78\", \"treat\", covar)\nout5 <- estimate_all(nsw_psid, \"re78\", \"treat\", covar)\nout6 <- estimate_all(nsw_cps_trim_match, \"re78\", \"treat\", covar)\nout7 <- estimate_all(nsw_psid_trim_match, \"re78\", \"treat\", covar)\n# print the result\na <- list(out4, out5, out6, out7)\nn <- nrow(out1)\nsav <- matrix(\"\", n+1, length(a)*3-1)\nfor (j in 1:length(a)) {\n    out <- a[[j]]\n    n <- nrow(out)\n    for (i in 2:(nrow(out)+1)) {\n        sav[i, j*3-2] <- sprintf(\"%.2f\", out[i-1, 1])\n        sav[i, j*3-1] <- paste0(\"(\", sprintf(\"%.2f\", out[i-1, 2]), \")\")\n    }\n}\nsav[1, 1] <- sprintf(\"%.2f\", out1[1, 1])\nsav[1, 2] <- paste0(\"(\", sprintf(\"%.2f\", out1[1, 2]), \")\")\nsav[1, 4] <- sprintf(\"%.2f\", out1[1, 1])\nsav[1, 5] <- paste0(\"(\", sprintf(\"%.2f\", out1[1, 2]), \")\")\nsav[1, 7] <- sprintf(\"%.2f\", out2[1, 1])\nsav[1, 8] <- paste0(\"(\", sprintf(\"%.2f\", out2[1, 2]), \")\")\nsav[1, 10] <- sprintf(\"%.2f\", out3[1, 1])\nsav[1, 11] <- paste0(\"(\", sprintf(\"%.2f\", out3[1, 2]), \")\")\ncolnames(sav) <- c(\"NSW-CPS1\", \"\", \"\", \"NSW-PSID1\", \"\", \"\", \"NSW-CPS1 (PS Trimmed) \", \"\", \"\", \"NSW-PSID1 (PS Trimmed)\", \"\")\nrownames(sav) <- c(\"Experimental Benchmark\", \"Difference-in-Means\", \"Regression\", \" Oaxaca Blinder\", \"GRF\", \"NN Matching\", \"PS Matching\", \"IPW\", \"CBPS\", \"Entropy Balancing\", \"DML-ElasticNet\", \"AIPW-GRF\")\nsav %>% knitr::kable(booktabs=TRUE, caption = \" Table A4 in the Supplementary Materials (SM), ATT Estimates: LaLonde Male Sample\")"},{"path":"original-lalonde-data-male-sample.html","id":"alternative-estimands-catt-and-qtet-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.6 Alternative Estimands: CATT and QTET","text":"","code":""},{"path":"original-lalonde-data-male-sample.html","id":"conditional-average-treatment-effect-on-the-treated-catt-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.6.1 Conditional Average Treatment Effect on the Treated (CATT)","text":"figures show CATT estimates using original LaLonde data (male sample).point scatter plots represents pair CATT estimates single unit: one experimental benchmark one observational method. Points lie 45-degree line (red line) cases observational experimental methods yield estimate.\nFigure 3.4: FIGURE A6. CATT Estimates LaLonde Data (Male Sample)\nNote: Scatterplots show CATT using experimental data (x-axis) nonexperimental data (y-axis) LaLonde (1986) (male sample). dot corresponds CATT estimate based covariate values treated unit, red cross symbolizes ATT estimates. every estimate, AIPW estimator employed, GRF approach estimating nuisance parameters. Different subfigures indicate various data comparisons: Subfigure : Compares Experimental LaLonde-CPS1. Subfigure B: Compares Experimental LaLonde-PSID1. Subfigure C: Compares trimmed Experimental (removing 30 treated units) trimmed NSW-CPS1. Subfigure D: Compares trimmed Experimental (removing 150 treated units) trimmed NSW-PSID1.Based untrimmed data, Subfigure B show wide dispersion points around 45-degree line. raises concern lack agreement experimental benchmark CATT estimates CPS1 PSID1 control groups.hand, trimming based propensity scores, dispersion points Subfigure C D concentrated around 45-degree line, indicating agreement experimental benchmark CATT estimates improved. comparison suggests trimming successfully reduced bias estimating treatment effect ensuring treated control groups similar covariates distributions.","code":"\n# estimate catt\ncatt.nsw <- catt(nsw, Y, treat, covar)\ncatt.cps <- catt(nsw_cps, Y, treat, covar)\ncatt.psid <- catt(nsw_psid, Y, treat, covar)\ncatt.cps.trim <- catt(nsw_cps_trim_match, Y, treat, covar)\ncatt.psid.trim <- catt(nsw_psid_trim_match, Y, treat, covar)\n# trimmed experimental data\ncatt.nsw.cps <- catt(nsw_trim_cps, Y, treat, covar)\ncatt.nsw.psid <- catt(nsw_trim_psid, Y, treat, covar)\npar(mfrow = c(2,2))\n# plot catt - \"CATT (Experimental)\" and \"CATT (CPS-Full)\"\ncatt1 <- catt.nsw$catt\natt1 <- catt.nsw$att[1]\ncatt2 <- catt.cps$catt\natt2 <- catt.cps$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (CPS-Full)\",\n          main = \"A. NSW-CPS1\", c(-8000, 8000))\n\n# plot catt - \"CATT (Experimental)\" and \"CATT (PSID-Full)\"\ncatt1 <- catt.nsw$catt\natt1 <- catt.nsw$att[1]\ncatt2 <- catt.psid$catt\natt2 <- catt.psid$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (PSID-Full)\",\n    main = \"B. NSW-PSID1\", c(-8000, 8000))\n\n# plot catt - \"CATT (Experimental)\" and \"CATT (CPS-Trimmed)\"\ncatt1 <- catt.nsw.cps$catt\natt1 <- catt.nsw.cps$att[1]\ncatt2 <- catt.cps.trim$catt\natt2 <- catt.cps.trim$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (CPS-Trimmed)\",\n    main = \"C. NSW-CPS1 Trimmed\", c(-8000, 8000))\n\n# plot catt - \"CATT (Experimental)\" and \"CATT (PSID-Trimmed)\"\ncatt1 <- catt.nsw.psid$catt\natt1 <- catt.nsw.psid$att[1]\ncatt2 <- catt.psid.trim$catt\natt2 <- catt.psid.trim$att[1]\nplot_catt(catt1, catt2, att1, att2, \"CATT (Experimental)\", \"CATT (PSID-Trimmed)\",\n    main = \"D. NSW-PSID1 Trimmed\", c(-8000, 8000))"},{"path":"original-lalonde-data-male-sample.html","id":"quantile-treatment-effect-on-the-treated-qtet-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.6.2 Quantile Treatment Effect on the Treated (QTET)","text":"Figures show quantile treatment effects treated original LaLonde male sample. QTET analysis helps us see along outcome distribution treatment less effective.\ncode consists two main parts:est_qte function estimates QTET.Plotting QTET: plot_qte function creates plots, separate plots data comparison (NSW-CPS, NSW-CPS trimmed, NSW-PSID, NSW-PSID trimmed).\nFigure 3.5: FIGURE A7. Quantile Treatment Effects: Experimental vs. Nonexperimental\nNote: Figures show QTET using experimental data (blue) nonexperimental data (red raw estimates black covariate-adjusted estimates). dot corresponds QTET estimate particular quantile, shaded areas represent bootstrapped 95% confidence intervals. Unadjusted models incorporate covariates adjustment models use full set covariates estimate propensity scores logit.","code":"\n# estimate qte (some of the following lines are not run due to computational limitation)\nqte.nsw <- est_qte(Y, treat, NULL, data = nsw)\nqte.nsw.cps <- est_qte(Y, treat, NULL, data = nsw_trim_cps)\nqte.nsw.psid <- est_qte(Y, treat, NULL, data = nsw_trim_psid)\n#qte.nsw_cps <- est_qte(Y, treat, covar, data = nsw_cps) # adjusted\n#qte.nsw_cps0 <- est_qte(Y, treat, NULL, data = nsw_cps) # unadjusted\nqte.nsw_cps.trim <- est_qte(Y, treat, covar, data = nsw_cps_trim_match) # adjusted\nqte.nsw_cps.trim0 <- est_qte(Y, treat, NULL, data = nsw_cps_trim_match) # unadjusted\n#qte.nsw_psid <- est_qte(Y, treat, covar, data = nsw_psid) # adjusted\n#qte.nsw_psid0 <- est_qte(Y, treat, NULL, data = nsw_psid) # unadjusted\nqte.nsw_psid.trim <- est_qte(Y, treat, covar, data = nsw_psid_trim_match) # adjusted\nqte.nsw_psid.trim0 <- est_qte(Y, treat, NULL, data = nsw_psid_trim_match) # unadjusted\n# plot qte\n\n#load the data\nload(\"data/qte_nsw.rds\")\n\npar(mfrow = c(2,2))\n# CPS\nplot_qte(qte.nsw_cps, qte.nsw_cps0, qte.nsw, main = \"NSW-CPS\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")\n\n## CPS trimmed\nplot_qte(qte.nsw_cps.trim, qte.nsw_cps.trim0, qte.nsw.cps, main = \"NSW-CPS (Trimmed)\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), \n    lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")\n\n# PSID\nplot_qte(qte.nsw_psid, qte.nsw_psid0, qte.nsw, main = \"NSW-PSID\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), \n    lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")\n\n# PSID trimmed\nplot_qte(qte.nsw_psid.trim, qte.nsw_psid.trim0, qte.nsw.psid, main = \"NSW-PSID (Trimmed)\", ylim = c(-25000, 15000))\nlegend(\"bottomleft\", legend = c(\"Experimental\", \"Unadjusted\", \"Adjusted\"), \n    lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = \"n\")"},{"path":"original-lalonde-data-male-sample.html","id":"sensitivity-analyses-1","chapter":"3 Original LaLonde Data (Male Sample)","heading":"3.7 Sensitivity Analyses","text":"sensitivity analyses using original LaLonde male sample, results depicted contour plots .\nFigure 3.6: FIGURE A3. Sensitivity Analyses Trimmed NSW-CPS1 NSW-PSID1\nanalyses suggest estimated training effect based trimmed NSW-CPS less sensitive potential confounders compared trimmed NSW-PSID.","code":"\npar(mfrow = c(1,2))\n\n## datasets to be used: nsw, nsw_trim_cps, nsw_trim_psid\nY <- \"re78\"\ntreat <- \"treat\"\ncovar <- c(\"age\", \"education\", \"black\", \"hispanic\", \"married\", \"nodegree\", \"re75\", \"u75\")\nbm <- c(\"re75\")\n\n# trimmed NSW-CPS data\nsens_ana(nsw_trim_cps, Y, treat, covar, bm, kd = 1:3)\n\n# trimmed NSW-PSID data\nsens_ana(nsw_trim_psid, Y, treat, covar, bm, kd = 1)"},{"path":"the-lottery-data.html","id":"the-lottery-data","chapter":"4 The Lottery Data","heading":"4 The Lottery Data","text":"","code":""},{"path":"the-lottery-data.html","id":"the-imbens-rubin-and-sacerdote-irs-lottery-data","chapter":"4 The Lottery Data","heading":"4.1 The Imbens, Rubin, and Sacerdote (IRS) Lottery Data","text":"now reanalyze lottery data Imbens, Rubin, Sacerdote (2001), carried original survey investigate impact size lottery prizes Massachusetts mid-1980s economic behavior lottery players. primary outcome post-winning labor earnings.three treatment control groups. control group, termed “non-winners,” consists 259 season ticket holders won small, one-time prize, ranging $100 $5,000 (essence, one-time, minor winners). treatment groups, labeled “big winners” (43 individuals) “small winners” (194 individuals), clinched major prize.","code":"\nload(\"data/lottery.RData\")\nd$tr <- d$winner\nd$tr1 <- ifelse(d$bigwinner == 1, 1, 0) # big winner\nd$tr2 <- ifelse(d$bigwinner == 0 & d$winner == 1, 1, 0) # small winner\nd$co <- ifelse(d$winner == 0, 1, 0) # control\nd$college <- ifelse(d$educ >= 16, 1, 0)"},{"path":"the-lottery-data.html","id":"assessing-overlap-2","chapter":"4 The Lottery Data","heading":"4.2 Assessing Overlap","text":"subsequent analysis, consider labor earnings seven post-lottery winning periods outcomes. denoted \\(Y_{,0}\\), …, \\(Y_{,6}\\), t = 0 represents year winning lottery—recall individuals control group also received modest, one-time prize year. treat labor earnings three years immediately preceding lottery win, .e., \\(Y_{,-3}\\), \\(Y_{,-2}\\), \\(Y_{,-1}\\), well average, placebo outcomes. labor earnings three years , .e., \\(Y_{,-6}\\), \\(Y_{,-5}\\), \\(Y_{,-4}\\), used covariates adjustment, alongside set time-invariant pre-lottery-winning variables. include number tickets purchased (tixbot), gender (male), employment status time winning (workthen), age lottery won (agew), total years education (educ), presence college degree (college).First, estimate overlap two treatment groups(big winner treatment group left & smaller winner treatment group right) control group. assess_overlap function conveniently present overlap control treated.\nFigure 4.1: FIGURE 7. SubfigureA:Big Winners vs Non-Winners. SubfigureB:Small Winners vs Non-Winners.\nfigures indicate propensity score distributions individuals treatment groups differ control group, propensity scores treatment groups still fall within support control group.","code":"\npar(mfrow = c(1,2))\n# select big winner\n\ns1 <- subset(d, tr1 == 1 | co == 1)\n\ns1$xearn.avg <- apply(s1[, paste0(\"xearn.\", 4:6)], 1, mean) # avg pre outcome\ns1$yearn.avg <- apply(s1[, paste0(\"yearn.\", 1:7)], 1, mean) # avg pst outcome\n\n# assess overlap\ntreat <- \"tr\"\ncovar <- c(\"tixbot\", \"male\", \"workthen\", \"agew\", \"educ\", \"college\",\n           \"xearn.1\", \"xearn.2\", \"xearn.3\", \"yearw\")\n\ns1_ps <- assess_overlap(data = s1, treat = treat, cov = covar, xlim = c(-5.5, 1), ylim = c(-0.4, 0.4), breaks = 30)\n\n# select small winner\n\ns2 <- subset(d, tr2 == 1 | co == 1)\n\ns2$xearn.avg <- apply(s2[, paste0(\"xearn.\", 4:6)], 1, mean) # avg pre outcome\ns2$yearn.avg <- apply(s2[, paste0(\"yearn.\", 1:7)], 1, mean) # avg pst outcome\n\n# assess overlap\n\ntreat <- \"tr\"\ncovar <- c(\"tixbot\", \"male\", \"workthen\", \"agew\", \"educ\", \"college\",\n           \"xearn.1\", \"xearn.2\", \"xearn.3\", \"yearw\")\n\ns2_ps <- assess_overlap(data = s2, treat = treat, cov = covar, xlim = c(-3, 3), ylim = c(-0.2, 0.2), breaks = 30)"},{"path":"the-lottery-data.html","id":"trimming-to-improve-overlap-2","chapter":"4 The Lottery Data","heading":"4.3 Trimming to Improve Overlap","text":"improve overlap, trim control group two treatment groups implementing 1:1 matching based propensity scores. , reassess overlap two trimmed datasets.\nFigure 4.2: FIGURE A9. SubfigureA:Big Winners vs Control. SubfigureB:Small Winners vs Control.\nhistograms, trimming improved overlap big winner treatment group small winner one. points consider.First, treatment control groups different covariate distributions, difficult achieve good overlap even trimming. plots suggest, big winner group likely control units similar treated units, making easier find matches.Second, small winner group’s log odds wide range trimming remain wide trimming. suggests small winner group might extreme values outliers present control group.","code":"\npar(mfrow = c(1,2))\n# matching\n  ## In the matching procedure, we don't need Y actually, so just pick one Y to satisfy function's requirement\n\ns1_ps_match <- psmatch(data = s1_ps, Y = \"yearn.2\", treat = treat, cov = covar)\n\n# assess overlap again\n\nss <- assess_overlap(data = s1_ps_match, treat = treat, cov = covar, xlim = c(-1,1), breaks = 30)\n\n\n# matching\n  ## In the matching procedure, we don't need Y actually, so just pick one Y to satisfy function's requirement\n\ns2_ps_match <- psmatch(data = s2_ps, Y = \"yearn.2\", treat = treat, cov = covar)\n\n# assess overlap again\n\nss <- assess_overlap(data = s2_ps_match, treat = treat, cov = covar, xlim = c(-3,3), breaks = 30)"},{"path":"the-lottery-data.html","id":"att-and-placebo-estimates","chapter":"4 The Lottery Data","heading":"4.4 ATT and Placebo Estimates","text":"Since trimming significantly improve overlap treated control, proceed original dataset.\nFigure 4.3: FIGURE 8. ATT Placebo Estimates: IRSData\nATT results presented table :Table 4.1:  Table A5 Supplementary Materials (SM), ATT Placebo Estimates: IRSDataUsing original data, table presents ATT estimates results placebo test using various estimators. Columns 1 2 compare “big winners” controls, columns 3 4 compare “small winners” control. Columns 1 3 display ATT estimates, outcome average annual labor earnings Year 0 Year 6. results placebo test reported columns 2 4, placebo outcome average annual labor earnings Year 3 Year 1. results indicate various methods produce consistent results, align findings reported original paper: winning large prize leads significant decrease labor income following years, averaging much $8,000 annually. contrast, winning smaller prize results modest decline, averaging approximately $3,000 per year. Notably, applying doubly robust estimators like AIPW-GRF, placebo test estimates hover near zero, reinforcing credibility unconfoundedness assumption.","code":"\n# prepare data again\n\nd$tr <- d$winner\nd$tr1 <- ifelse(d$bigwinner == 1, 1, 0) # big winner\nd$tr2 <- ifelse(d$bigwinner == 0 & d$winner == 1, 1, 0) # small winner\nd$co <- ifelse(d$winner == 0, 1, 0) # control\nd$college <- ifelse(d$educ >= 16, 1, 0)\n\ncolnames(d)[9:14] <- paste0(\"x\", 1:6)\ncolnames(d)[15:21] <- paste0(\"y\", 1:7)\nd$earnings_1yr_before <- d$x6\n\nd$xearn.avg <- apply(d[, paste0(\"x\", 4:6)], 1, mean) # avg pre outcome\nd$yearn.avg <- apply(d[, paste0(\"y\", 1:7)], 1, mean) # avg pst outcome\n\ns1 <- subset(d, tr1 == 1 | co == 1) # big winner\ns2 <- subset(d, tr2 == 1 | co == 1) # small winner\n\n# estimate and placebo analyses\n\ncovar <- c(\"tixbot\", \"male\", \"workthen\", \"agew\", \"educ\", \"college\",\n           \"x1\", \"x2\", \"x3\", \"yearw\")\n\n# big winners\nset.seed(1234)\nout1 <- estimate_all(s1, \"yearn.avg\", \"tr\", covar)\n#out1\nout2 <- estimate_all(s1, \"xearn.avg\", \"tr\", covar)\n#out2\n# small winners\nout3 <- estimate_all(s2, \"yearn.avg\", \"tr\", covar)\n#out3\nout4 <- estimate_all(s2, \"xearn.avg\", \"tr\", covar)\n#out4\npar(mfrow = c(4,1))\nylim <- c(-20, 20)\n\nplot_coef(out1, ylim = ylim, main = \"Big Winner vs. Non-Winner\", main.pos = 3)\n\nplot_coef(out2, ylim = ylim, main = \"Big Winner vs. Non-Winner: Placebo Test\", main.pos = 3)\n\nplot_coef(out3, ylim = ylim, main = \"Small Winner vs. Non-Winner\", main.pos = 3)\n\nplot_coef(out4, ylim = ylim, main = \"Small Winner vs. Non-Winner: Placebo Test\", main.pos = 3)\n# print the result\na <- list(out1, out2, out3, out4)\nn <- nrow(out1)\nsav <- matrix(\"\", n, length(a)*3-1)\nfor (j in 1:length(a)) {\n    out <- a[[j]]\n    n <- nrow(out)\n    for (i in 1:(nrow(out))) {\n        sav[i, j*3-2] <- sprintf(\"%.2f\", out[i, 1])\n        sav[i, j*3-1] <- paste0(\"(\", sprintf(\"%.2f\", out[i, 2]), \")\")\n    }\n}\ncolnames(sav) <- c(\"Big Prize: Post-Winning Average Earning\", \"\", \"\", \"Big Prize: Pre-Winning Average Earning\", \"\", \"\", \"Small Prize: Post-Winning Average Earning\", \"\", \"\", \"Small Prize: Pre-Winning Average Earning\", \"\")\nrownames(sav) <- c(\"Difference-in-Means\", \"Regression\", \" Oaxaca Blinder\", \"GRF\", \"NN Matching\", \"PS Matching\", \"IPW\", \"CBPS\", \"Entropy Balancing\", \"DML-ElasticNet\", \"AIPW-GRF\")\nsav %>% knitr::kable(booktabs=TRUE, caption = \" Table A5 in the Supplementary Materials (SM), ATT and Placebo Estimates: IRSData\")"},{"path":"the-lottery-data.html","id":"att-visualization","chapter":"4 The Lottery Data","heading":"4.5 ATT Visualization","text":"tables can enumerate standard errors estimate, graphical visualization offer intuitive presentation findings. facilitate clearer understanding, calculate compare ATT estimates derived original matched dataset. following code computes difference--means alongside AIPW-GRF estimates.ATT Results Visualization:\nFigure 4.4: 9() (B). ATT Estimates: IRS Data\nfigures show comparison “big winners” “non-winners,” AIPW using original trimmed data produces estimates similar simple difference--means estimator, suggesting minimal selection bias two groups. hand, comparing “small winners” “non-winners,” estimates AIPW difference--means diverge. However, findings former much credible latter difference--means fare well placebo tests, whereas former yields placebo estimates nearly 0.","code":"\n# big winners\ns <- s1_ps\ns2 <- s1_ps_match\ncovar <- c(\"tixbot\", \"male\", \"workthen\", \"agew\", \"educ\", \"college\", \n    \"xearn.1\", \"xearn.2\", \"xearn.3\", \"yearw\")\ntreat <- \"tr\"\n# full dataset\noutcomes <- c(paste0(\"xearn.\", 1:6), paste0(\"yearn.\", 1:7))\nest <- vector(\"list\", length(outcomes))\nnames(est) <- outcomes\nfor (i in 1:length(outcomes)) {\n    est[[i]] <- estimate_all(s, outcomes[i], \"tr\", covar,\n    methods = c(\"diff\", \"aipw_grf\"))\n    #cat(i, \"\\n\")\n}\n# matched dataset\nest2 <- vector(\"list\", length(outcomes))\nnames(est2) <- outcomes\nfor (i in 1:length(outcomes)) {\n    est2[[i]] <- estimate_all(s2, outcomes[i], \"tr\", covar,\n    methods = c(\"diff\", \"aipw_grf\"))\n    #cat(i, \"\\n\")\n}\n\n# Small winners\ns <- s2_ps\ns2 <- s2_ps_match\ncovar <- c(\"tixbot\", \"male\", \"workthen\", \"agew\", \"educ\", \"college\", \n    \"xearn.1\", \"xearn.2\", \"xearn.3\", \"yearw\")\ntreat <- \"tr\"\n# full dataset\noutcomes <- c(paste0(\"xearn.\", 1:6), paste0(\"yearn.\", 1:7))\nest3 <- vector(\"list\", length(outcomes))\nnames(est3) <- outcomes\nfor (i in 1:length(outcomes)) {\n    est3[[i]] <- estimate_all(s, outcomes[i], \"tr\", covar,\n    methods = c(\"diff\", \"aipw_grf\"))\n    #cat(i, \"\\n\")\n}\n# matched dataset\nest4 <- vector(\"list\", length(outcomes))\nnames(est4) <- outcomes\nfor (i in 1:length(outcomes)) {\n    est4[[i]] <- estimate_all(s2, outcomes[i], \"tr\", covar,\n    methods = c(\"diff\", \"aipw_grf\"))\n    #cat(i, \"\\n\")\n}\npar(mfrow = c(2,1))\npar(mar = c(4, 4, 1, 2))\nplot(1, xlim = c(3.7, 13.3), ylim = c(-20, 10), type = \"n\", axes = FALSE, \n    ylab = \"Effects on Earnings (thousand USD)\", xlab = \"Year Relative to Winning\")\nbox(); axis(2)\naxis(1, at = 4:13, labels = c(-3:6))\nabline(h = 0, v= 6.5, col = \"gray60\", lty = 2, lwd = 2)\nfor (i in 4:13) {\n    # full dataset with DIM\n    lines(c(i-0.15, i-0.15), est[[i]][1,3:4], lty = 1, lwd = 2, col = \"grey60\") # CI\n    points(i-0.15, est[[i]][1,1], pch = 18, col = \"grey60\", cex = 1.2)  # Coef \n    # full dataset\n    lines(c(i, i), est[[i]][2,3:4], lwd = 2) # CI\n    points(i, est[[i]][2,1], pch = 16)  # Coef \n    # matched dataset\n    lines(c(i+0.15, i+0.15), est2[[i]][2,3:4], col = \"maroon\", lwd = 1.5) # CI\n    points(i+0.15, est2[[i]][2,1], col = \"maroon\", pch = 17)  # Coef  \n}\nlegend(\"topright\", legend = c(\"DIM,  Full (43: 259)\", \"AIPW, Full (43: 259)\", \n    \"AIPW, PS Matched (43: 43)\"), lwd = 2,\n    lty = c(1, 1, 1), pch = c(18, 16, 17), \n    col = c(\"grey50\", \"black\", \"maroon\"), bty = \"n\")\n\npar(mar = c(4, 4, 1, 2))\nplot(1, xlim = c(3.7, 13.3), ylim = c(-20, 10), type = \"n\", axes = FALSE, \n    ylab = \"Effects on Earnings (thousand USD)\", xlab = \"Year Relative to Winning\")\nbox(); axis(2)\naxis(1, at = 4:13, labels = c(-3:6))\nabline(h = 0, v= 6.5, col = \"gray60\", lty = 2, lwd = 2)\nfor (i in 4:13) {\n    # full dataset with DIM\n    lines(c(i-0.15, i-0.15), est3[[i]][1,3:4], lty = 1, lwd = 2, col = \"grey60\") # CI\n    points(i-0.15, est3[[i]][1,1], pch = 18, col = \"grey60\", cex = 1.2)  # Coef \n    # full dataset\n    lines(c(i, i), est3[[i]][2,3:4], lwd = 2) # CI\n    points(i, est3[[i]][2,1], pch = 16)  # Coef \n    # matched dataset\n    lines(c(i+0.15, i+0.15), est4[[i]][2,3:4], col = \"maroon\", lwd = 1.5) # CI\n    points(i+0.15, est4[[i]][2,1], col = \"maroon\", pch = 17)  # Coef  \n}\nlegend(\"topright\", legend = c(\"DIM,  Full (194: 259)\", \"AIPW, Full (194: 259)\", \n    \"AIPW, PS Matched (194: 194)\"), lwd = 2,\n    lty = c(1, 1, 1), pch = c(18, 16, 17), \n    col = c(\"grey50\", \"black\", \"maroon\"), bty = \"n\")"},{"path":"the-lottery-data.html","id":"catt-estimates-and-visualization","chapter":"4 The Lottery Data","heading":"4.6 CATT Estimates and Visualization","text":"assess effects treatment, proceed evaluate CATT, determined covariate values treated units. assessment conducted using AIPW-GRF method ten outcomes within original big winners original small winners samples.plots, years winning (Years -3, -2, -1) serve placebo test period. years leading win, CATT estimates expected around 0, suggest treatment (winning lottery) effect years. width violins indicates density effect estimates — wider sections violin plot suggest data points present effect size.\nFigure 4.5: 9(C) (D). CATT Estimates: IRS Data\nBeyond providing corroborative evidence placebo tests (.e., CATT estimates align closely 0 years leading win expected), figures also reveal substantial treatment effect heterogeneity among lottery winners. Post-Winning Years (Years 1 6), effect winning earnings changes time.Notably, distributions CATT many post-winning years appear bimodal. observe two peaks distribution effects, suggest two different common outcomes reactions winning among two treated groups.","code":"\ndata <- s1_ps\ntreat <- \"tr\"\nntr <- sum(data[, treat] == 1)\ntau <- matrix(NA, ntr, length(outcomes))\natt <- rep(NA, ntr)\nfor (i in 1:length(outcomes)) {\n    Y <- outcomes[i]\n    catt.out <- catt(data, Y, treat, covar)\n    tau[, i] <- catt.out$catt\n    att[i] <- catt.out$att[1]     \n    #cat(i, \"\\n\")\n}\n\ndata <- s2_ps\ntreat <- \"tr\"\nntr <- sum(data[, treat] == 1)\ntau2 <- matrix(NA, ntr, length(outcomes))\natt2 <- rep(NA, ntr)\nfor (i in 1:length(outcomes)) {\n    Y <- outcomes[i]\n    catt.out <- catt(data, Y, treat, covar)\n    tau2[, i] <- catt.out$catt\n    att2[i] <- catt.out$att[1]     \n    #cat(i, \"\\n\")\n}\npar(mfrow = c(2,1))\npar(mar = c(4, 4, 1, 2))\nplot(1, xlim = c(3.7, 13.3), ylim = c(-20, 10), type = \"n\", axes = FALSE, \n    ylab = \"Effects on Earnings (thousand USD)\", xlab = \"Year Relative to Winning\")\nbox(); axis(2)\naxis(1, at = 4:13, labels = c(-3:6))\nabline(h = 0, v= 6.5, col = \"gray60\", lty = 2, lwd = 1.5)\nfor (i in 4:length(outcomes)) {\n    dens <- density(tau[,i], bw = 0.5)\n    polygon(i + dens$y, dens$x, col = \"#AAAAAA50\", border = NA)\n    lines(i + dens$y, dens$x, lwd = 1) \n    points(i+0.01,  att[i], pch = 16, cex = 0.8)  # Coef\n}\n\npar(mar = c(4, 4, 1, 2))\nplot(1, xlim = c(3.7, 13.3), ylim = c(-20, 10), type = \"n\", axes = FALSE, \n    ylab = \"Effects on Earnings (thousand USD)\", xlab = \"Year Relative to Winning\")\nbox(); axis(2)\naxis(1, at = 4:13, labels = c(-3:6))\nabline(h = 0, v= 6.5, col = \"gray60\", lty = 2, lwd = 1.5)\nfor (i in 4:length(outcomes)) {\n    dens <- density(tau2[,i], bw = 0.5)\n    polygon(i + dens$y, dens$x, col = \"#AAAAAA50\", border = NA)\n    lines(i + dens$y, dens$x, lwd = 1) \n    points(i+0.01,  att2[i], pch = 16, cex = 0.8)  # Coef\n}"},{"path":"the-lottery-data.html","id":"summary-1","chapter":"4 The Lottery Data","heading":"4.7 Summary","text":"Overall, find lottery study, unconfoundedness assumption can empirically validated placebo tests, bolstering credibility causal estimates. Importantly, unconfoundedness assumption much believable study LaLonde case inherent randomization lotteries played key role treatment assignment, supplementary covariates help account discrepancies treatment control groups stemming challenges like differential responses survey. inclusion six preceding outcomes also proves invaluable, likely explain selection mechanism highly correlated outcome variables; moreover, also serve good candidates placebo outcomes, given comparability outcomes.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
